{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81fe0b4-f5d7-4a9a-ab27-c2766cf3cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from load_csv import CSV_Loader\n",
    "from configs.configuration import general_config, dataset_config\n",
    "import pandas as pd\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30a34e2-f99d-42b7-a403-bae941e49493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _Loader:\n",
    "    \"\"\"\n",
    "    Interface that loads all the data into the memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            pass\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(\"Error occured in initialization of _Loader interface due to \", e)\n",
    "                \n",
    "        finally:\n",
    "            display(\"Loader Interface initialized\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_file():\n",
    "        raise NotImplementedError    \n",
    "\n",
    "\n",
    "class CSV_Loader(_Loader):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            super().__init__()\n",
    "        \n",
    "        except Exception as e:\n",
    "            display(\"Error occured in initialization of CSV_Loader class due to \", e)\n",
    "                \n",
    "        finally:\n",
    "            display(\"CSV_Loader initialized\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_file(csv_file_path,\n",
    "                   index_column_name=None,\n",
    "                   _nrows=None,\n",
    "                   _iterator=True,\n",
    "                   _chunksize=100000):\n",
    "        try:\n",
    "            tp = pd.read_csv(csv_file_path, nrows=_nrows, index_col=index_column_name, iterator=_iterator, chunksize=_chunksize) ## loading data in chunks reduces 90 percent execution time \n",
    "            df = pd.concat(tp, ignore_index=False)\n",
    "            df.info(verbose=False, memory_usage=\"deep\")\n",
    "            return df  \n",
    "        \n",
    "        except Exception as e:\n",
    "            display(\"Error occured in _load_file method of CSV_Loader class due to \", e)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_file_via_dask(csv_file_path,\n",
    "                            fetch_houses):\n",
    "        try:\n",
    "            display(f\"Loading specified houses: {fetch_houses}\")\n",
    "            return [{i: dd.read_csv(f'{csv_file_path}House_{i}.csv')} for i in fetch_houses]\n",
    "        \n",
    "        except Exception as e:\n",
    "            display(\"Error occured in _load_file_via_dask method of CSV_Loader class due to \", e)\n",
    "            \n",
    "            \n",
    "    ###### appliance wise Dict Reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db187f5-3273-4e83-9543-1eaf54b56409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(readme_file):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        display(f'Loading the readme files specified: {readme_file}')\n",
    "        with open(readme_file) as f:\n",
    "            content = f.readlines()\n",
    "        ls = {}\n",
    "        for i, s in enumerate(content):\n",
    "            if 'House' in s.capitalize():\n",
    "                keys, appliances = [], []\n",
    "                house = s.split()[1]\n",
    "                for indx in range(1, 6):\n",
    "                    if content[i+indx] == '\\t!NOTES\\n':\n",
    "                        break\n",
    "                    else:\n",
    "                        target = [value.split('.') for value in [value for value in content[i+indx].split(',') if value != '\\n']]\n",
    "                        indx = [target_value[0] for target_value in target]\n",
    "                        values = [target_value[1] for target_value in target]\n",
    "                        keys.append([target_value[0] for target_value in target])\n",
    "                        appliances.append([target_value[1] for target_value in target])\n",
    "                ls.update({house: {'appliances':  [item.split('\\n')[0] for sublist in appliances for item in sublist], 'keys': [item for sublist in keys for item in sublist]}})\n",
    "        return ls\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        display(\"Error occured in parser method due to \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4629c85d-1fe0-48af-9c87-d5a727f5bd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loader Interface initialized'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'CSV_Loader initialized'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ob = CSV_Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908cd34c-a862-4fcb-8335-1b7100fb8fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loading specified houses: [1, 3]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 71.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collective_dataset = ob._load_file_via_dask(csv_file_path=general_config['DATA_FOLDER'], fetch_houses=dataset_config['HOUSES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e78de71-a51b-4f81-af82-096829d0689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: Dask DataFrame Structure:\n",
       "                   Time   Unix Aggregate Appliance1 Appliance2 Appliance3 Appliance4 Appliance5 Appliance6 Appliance7 Appliance8 Appliance9\n",
       "  npartitions=7                                                                                                                            \n",
       "                 object  int64     int64      int64      int64      int64      int64      int64      int64      int64      int64      int64\n",
       "                    ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
       "  ...               ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
       "                    ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
       "                    ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
       "  Dask Name: read-csv, 7 tasks},\n",
       " {3: Dask DataFrame Structure:\n",
       "                   Time   Unix Aggregate Appliance1 Appliance2 Appliance3 Appliance4 Appliance5 Appliance6 Appliance7 Appliance8 Appliance9\n",
       "  npartitions=7                                                                                                                            \n",
       "                 object  int64     int64      int64      int64      int64      int64      int64      int64      int64      int64      int64\n",
       "                    ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
       "  ...               ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
       "                    ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
       "                    ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
       "  Dask Name: read-csv, 7 tasks}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collective_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9130db5-f180-4468-bae4-a31c5ea18d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loading the readme files specified: data/refit/REFIT_Readme.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys_of_appliances = parser(general_config['README_FILE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0561167c-3bac-4ff3-935e-48e42791c431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Dask DataFrame Structure:\n",
      "                 Time   Unix Aggregate Appliance1 Appliance2 Appliance3 Appliance4 Appliance5 Appliance6 Appliance7 Appliance8 Appliance9\n",
      "npartitions=7                                                                                                                            \n",
      "               object  int64     int64      int64      int64      int64      int64      int64      int64      int64      int64      int64\n",
      "                  ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
      "...               ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
      "                  ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
      "                  ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
      "Dask Name: read-csv, 7 tasks\n",
      "{'appliances': ['Aggregate', 'Fridge', 'Chest Freezer', 'Upright Freezer', 'Tumble Dryer', 'Washing Machine', 'Dishwasher', 'Computer Site', 'Television Site', 'Electric Heater'], 'keys': ['0', ' 1', ' 2', ' 3', ' 4', '5', ' 6', ' 7', ' 8', ' 9']}\n",
      "3\n",
      "Dask DataFrame Structure:\n",
      "                 Time   Unix Aggregate Appliance1 Appliance2 Appliance3 Appliance4 Appliance5 Appliance6 Appliance7 Appliance8 Appliance9\n",
      "npartitions=7                                                                                                                            \n",
      "               object  int64     int64      int64      int64      int64      int64      int64      int64      int64      int64      int64\n",
      "                  ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
      "...               ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
      "                  ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
      "                  ...    ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...\n",
      "Dask Name: read-csv, 7 tasks\n",
      "{'appliances': ['Aggregate', 'Toaster', 'Fridge-Freezer', 'Freezer', 'Tumble Dryer', 'Dishwasher', 'Washing Machine', 'Television', 'Microwave', 'Kettle'], 'keys': ['0', ' 1', ' 2', ' 3', ' 4', '5', ' 6', ' 7', ' 8', ' 9']}\n"
     ]
    }
   ],
   "source": [
    "# mapping_keys = [key for house_dict in collective_dataset for key in house_dict.keys()]\n",
    "# mapping_keys\n",
    "mapping_keys = []\n",
    "\n",
    "for house_dict in collective_dataset:\n",
    "    for key in house_dict.keys():\n",
    "        print(key)\n",
    "        print(house_dict[key])\n",
    "        print(keys_of_appliances[str(key)])\n",
    "#     mapping_key = house_dict.keys()\n",
    "\n",
    "# mapping_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7defdebc-2ac3-44e7-9172-625be8fde922",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_of_appliances['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f8a7d-010b-4bcf-b293-3e9166921eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for house in range(0, len(collective_dataset)):\n",
    "    print(house)\n",
    "#     print(d)\n",
    "    for i in range(1, len(keys_of_appliances[str(house+1)]['appliances'])):\n",
    "        collective_dataset[house].rename({f'Appliance{i}': nested_dict[str(house+1)]['appliances'][i]}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036d284-1556-44c1-963a-aa737e700598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for house_dict in collective_dataset:\n",
    "    print(type(house_dict))\n",
    "#     print(house_dict)\n",
    "#     collective_dataset\n",
    "    for key in house_dict.keys:\n",
    "        print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d224a9d-b8a3-4fca-a74d-ef458f0e3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d1 = collective_dataset[0].compute().set_index('Time')\n",
    "d3 = collective_dataset[1].compute().set_index('Time')\n",
    "d4 = collective_dataset[2].compute().set_index('Time')\n",
    "\n",
    "# d1 = collective_dataset[0].compute()\n",
    "# d3 = collective_dataset[1].compute()\n",
    "# d4 = collective_dataset[2].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e238689-86d8-4dfd-8ade-fb313623823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collective_dataset[0].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df901e34-17c9-49b1-bd14-a36b45292ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ob._load_file(csv_file_path=dataset_config['REFIT_DATA_PATH'], index_column_name='Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdc76d-095f-4eb1-b06e-ec392afef112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "files = glob.glob(dataset_config['REFIT_DATA_FOLDER'] + '*' + dataset_config['REFIT_DATA_TYPE'])\n",
    "print(files)\n",
    "# data = [pd.read_csv(file, index_col='Time') for file in files]\n",
    "# data\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa02d5c-b66c-4bff-8b71-8acd7c36a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a8202-f799-456d-947c-9d2448005b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c8de9-fc7a-4b10-a043-27d3a7b812be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = pd.read_csv(files[0], index_col='Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07795f84-1509-46ee-bd0f-a381495d5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba75943-883e-4ec8-af1d-2277dd65e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eeb333-d5c6-415a-96e6-b42ed75890b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data1 = dask.dataframe.read_csv(files[0])\n",
    "data2 = dask.dataframe.read_csv(files[1])\n",
    "data3 = dask.dataframe.read_csv(files[2])\n",
    "data4 = dask.dataframe.read_csv(files[3])\n",
    "\n",
    "df1=data1.compute()\n",
    "df2=data2.compute()\n",
    "df3=data3.compute()\n",
    "df4=data4.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c5869-0882-4f6a-b2ea-5ac2d3142e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02b5ea-8d12-4c7b-9fb8-191e3f806a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f1b65-7346-4d73-8fd6-4ca0cd690941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, file='d://sample.log')\n",
    "\n",
    "def hypotenuse(a, b):\n",
    "    \"\"\"Compute the hypotenuse\"\"\"\n",
    "    return (a**2 + b**2)**0.5\n",
    "\n",
    "kwargs = {'a':3, 'b':4, 'c':hypotenuse(3, 4)}\n",
    "\n",
    "logging.debug(\"a = {a}, b = {b}\".format(**kwargs))\n",
    "logging.info(\"Hypotenuse of {a}, {b} is {c}\".format(**kwargs))\n",
    "logging.warning(\"a={a} and b={b} are equal\".format(**kwargs))\n",
    "logging.error(\"a={a} and b={b} cannot be negative\".format(**kwargs))\n",
    "logging.critical(\"Hypotenuse of {a}, {b} is {c}\".format(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c015f31-2fcc-49e1-bac4-5a8f4c6983f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import parser\n",
    "print(\"Program to demonstrate parser module in Python\")\n",
    "print(\"\\n\")\n",
    "exp = \"5 + 8\"\n",
    "print(\"The given expression for parsing is as follows:\")\n",
    "print(exp)\n",
    "print(\"\\n\")\n",
    "print(\"Parsing of given expression results as: \")\n",
    "st = parser.expr(exp)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92a994-8e8d-4500-9f61-34c3ff1e2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "nested_dict  = pickle.load( open(\"file.pkl\", \"rb\") )\n",
    "d = [d1, d3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc84f5-2602-4fc7-9333-908c5eca0e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ee332-8726-476a-8fe4-cfa192000ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc2fff-21ad-4c15-a719-4a4f9295e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff5105-e0e9-4972-b2d5-90bfe10f7264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
